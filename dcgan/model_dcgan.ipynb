{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan 31 15:11:49 2019\n",
    "\n",
    "@author: JM\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "img = cv2.imread('./data/celebA/000003.jpg', cv2.IMREAD_COLOR)\n",
    "print(img.shape)\n",
    "\n",
    "res = cv2.resize(img,(64,64), interpolation = cv2.INTER_AREA)\n",
    "print(res.shape)\n",
    "\n",
    "cv2.imshow('image',res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization node\n",
    "class batch_norm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name,\n",
    "                                            reuse=tf.AUTO_REUSE  # if tensorflow vesrion < 1.4, delete this line\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "img_shape = [64,64,3]\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4x4 filter\n",
    "G_W1 = tf.Variable(tf.truncated_normal([4, 4, 1024, 100], stddev=0.02), name=\"G_W1\")\n",
    "#batch-norm\n",
    "G_bn1 = batch_norm(name=\"G_bn1\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W2 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='G_W2')\n",
    "#batch-norm\n",
    "G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='G_W3')\n",
    "#batch-norm\n",
    "G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W4 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='G_W4')\n",
    "#batch-norm\n",
    "G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W5 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='G_W5')\n",
    "\n",
    "\n",
    " #4x4 filter\n",
    "D_W1 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='D_W1')\n",
    "\n",
    " #4x4 filter\n",
    "D_W2 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='D_W2')\n",
    "#batch-norm\n",
    "D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='D_W3')\n",
    "#batch_norm\n",
    "D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W4 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='D_W4')\n",
    "#batch_norm\n",
    "D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W5 = tf.Variable(tf.truncated_normal([4, 4, 1024, 1], stddev=0.02), name='D_W5')\n",
    "\n",
    "D_var_list = [D_W1, D_W2, D_W3, D_W4, D_W5]\n",
    "G_var_list = [G_W1, G_W2, G_W3, G_W4, G_W5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(z):\n",
    "    #generate\n",
    "\n",
    "    #1x1x100\n",
    "    input_ = tf.reshape(z,[batch_size,1,1,100]) \n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W1 = tf.Variable(tf.truncated_normal([4, 4, 1024, 100], stddev=0.02), name=\"G_W1\")\n",
    "    #batch-norm\n",
    "    #G_bn1 = batch_norm(name=\"G_bn1\")\n",
    "\n",
    "    #1x1x100 -> 4x4x1024\n",
    "    layer_1 = tf.nn.conv2d_transpose(input_ \n",
    "                                     ,G_W1\n",
    "                                     ,output_shape=[batch_size,4,4,1024]\n",
    "                                     ,strides=[1,4,4,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_1 = tf.nn.relu(G_bn1(layer_1))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W2 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='G_W2')\n",
    "    #batch-norm\n",
    "    #G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "    #4x4x1024 -> 8x8x512\n",
    "    layer_2 = tf.nn.conv2d_transpose(layer_1\n",
    "                                    ,G_W2\n",
    "                                    ,output_shape=[batch_size,8,8,512]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_2 = tf.nn.relu(G_bn2(layer_2))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='G_W3')\n",
    "    #batch-norm\n",
    "    #G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "    #8x8x512 -> 16x16x256\n",
    "    layer_3 = tf.nn.conv2d_transpose(layer_2\n",
    "                                    ,G_W3\n",
    "                                    ,output_shape=[batch_size,16,16,256]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_3 = tf.nn.relu(G_bn3(layer_3))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W4 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='G_W4')\n",
    "    #batch-norm\n",
    "    #G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "    #16x16x256 -> 32x32x128\n",
    "    layer_4 = tf.nn.conv2d_transpose(layer_3\n",
    "                                    ,G_W4\n",
    "                                    ,output_shape=[batch_size,32,32,128]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_4 = tf.nn.relu(G_bn4(layer_4))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W5 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='G_W5')\n",
    "\n",
    "    #32x32x128 -> 64x64x3\n",
    "    layer_5 = tf.nn.conv2d_transpose(layer_4\n",
    "                                    ,G_W5\n",
    "                                    ,output_shape=[batch_size,64,64,3]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    output_ = tf.nn.tanh(layer_5)\n",
    "\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminate(img):\n",
    "    #4x4 filter\n",
    "    #D_W1 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='D_W1')\n",
    "\n",
    "    #64x64x3 -> 32x32x128\n",
    "    layer_1 = tf.nn.conv2d(img\n",
    "                           ,D_W1\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1,alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W2 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='D_W2')\n",
    "    #batch-norm\n",
    "    #D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "    #32x32x128 -> 16x16x256\n",
    "    layer_2 = tf.nn.conv2d(layer_1\n",
    "                           ,D_W2\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_2 = tf.nn.leaky_relu(D_bn2(layer_2),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='D_W3')\n",
    "    #batch_norm\n",
    "    #D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "    #16x16x256 -> 8x8x512\n",
    "    layer_3 = tf.nn.conv2d(layer_2\n",
    "                           ,D_W3\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_3 = tf.nn.leaky_relu(D_bn3(layer_3),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W4 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='D_W4')\n",
    "    #batch_norm\n",
    "    #D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "    #8x8x512 -> 4x4x1024\n",
    "    layer_4 = tf.nn.conv2d(layer_3\n",
    "                           ,D_W4\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_4 = tf.nn.leaky_relu(D_bn4(layer_4),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W5 = tf.Variable(tf.truncated_normal([4, 4, 1024, 1], stddev=0.02), name='D_W5')\n",
    "    \n",
    "    #4x4x1024 -> 1x1x1\n",
    "    layer_5 = tf.nn.conv2d(layer_4,D_W5,strides=[1,4,4,1],padding='SAME')\n",
    "    layer_5 = tf.reshape(layer_5,[batch_size,1])\n",
    "    \n",
    "    #percentage\n",
    "    output_ = tf.nn.sigmoid(layer_5)\n",
    "    \n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.placeholder(tf.float32,[batch_size,100])\n",
    "img_real = tf.placeholder(tf.float32,[batch_size]+img_shape)\n",
    "\n",
    "img_fake = generate(noise)\n",
    "\n",
    "#image real,fake inspection\n",
    "d_real = discriminate(img_real)\n",
    "d_fake = discriminate(img_fake)\n",
    "\n",
    "#d_cost want d_real to get bigger\n",
    "#d_cost want d_fake to get smaller\n",
    "d_cost = tf.reduce_mean(tf.log(d_real) + tf.log(1 - d_fake))\n",
    "#g_cost want d_fake to get bigger\n",
    "g_cost = tf.reduce_mean(tf.log(d_fake))\n",
    "\n",
    "#train\n",
    "d_train = tf.train.AdamOptimizer(learning_rate=learning_rate ,beta1=0.5).minimize(-d_cost ,var_list = D_var_list)\n",
    "g_train = tf.train.AdamOptimizer(learning_rate=learning_rate ,beta1=0.5).minimize(-g_cost ,var_list = G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
